{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23282cea-9b37-465f-a3f9-7e8caf25ce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-09 00:40:37 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "      from .autonotebook import tqdm as notebook_tqdm\n",
      "    \n",
      "[NeMo W 2025-05-09 00:40:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "import subprocess\n",
    "\n",
    "from nemo.collections.llm import api\n",
    "from nemo.collections.llm.evaluation.api import EvaluationConfig, EvaluationTarget\n",
    "from nemo.utils import logging\n",
    "\n",
    "logging.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf964980-69ba-447d-a6d8-1412726c768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this variable to point to your checkpoint\n",
    "# this notebook uses https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/llama-3_2-1b-instruct\n",
    "CHECKPOINT_PATH = \"/checkpoints/llama-3_2-1b-instruct_v2.0\"\n",
    "\n",
    "# if you are not using NeMo FW container, modify this path to point to scripts directory\n",
    "SCRIPTS_PATH = \"/opt/NeMo/scripts\"\n",
    "\n",
    "# modify this path if you would like to save results in a different directory\n",
    "WORKSPACE = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca87531-5e91-4857-a06f-b2cac4b6f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_script = f\"{SCRIPTS_PATH}/deploy/nlp/deploy_in_fw_oai_server_eval.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ae4669-6218-47d9-9a02-70c24fbb25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_process = subprocess.Popen(\n",
    "    [\"python\", deploy_script, \"--nemo_checkpoint\", CHECKPOINT_PATH], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f35cf1-3fa3-4ceb-8162-234edb2f2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://0.0.0.0:8886\"\n",
    "model_name = \"triton_model\"\n",
    "\n",
    "completions_url = f\"{base_url}/v1/completions/\"\n",
    "chat_url = f\"{base_url}/v1/chat/completions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250733b3-fbe2-4da3-bb18-6f357331c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 00:40:47 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:40:47 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:40:49 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:40:49 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:40:51 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:40:51 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:40:53 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:40:53 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:40:55 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:40:55 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:40:57 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:40:57 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-09 00:40:57 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 00:40:59 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:40:59 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:41:01 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:01 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Enabling Flash Decode for in-framework inference\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Apply rope scaling with factor=32.0, low_freq_factor=1.0, high_freq_factor=4.0, old_context_len=8192.\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1235814400\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Doing selective restore from RestoreConfig(path=PosixPath('/checkpoints/llama-3_2-1b-instruct_v2.0'), adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "[NeMo I 2025-05-09 00:41:02 nemo_logging:393] Using <megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x1552b0f0ee70> dist-ckpt load strategy.\n",
      "[NeMo I 2025-05-09 00:41:03 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:03 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:41:05 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:05 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:41:05 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1746776462.890s : Time spent in load_checkpoint: 2.522s\n",
      "[NeMo I 2025-05-09 00:41:05 nemo_logging:393] Restoring model weights from RestoreConfig(path=PosixPath('/checkpoints/llama-3_2-1b-instruct_v2.0'), adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n",
      "[NeMo I 2025-05-09 00:41:05 nemo_logging:393] Finished restoring from RestoreConfig(path=PosixPath('/checkpoints/llama-3_2-1b-instruct_v2.0'), adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n",
      "[NeMo I 2025-05-09 00:41:05 nemo_logging:393] Triton deploy function will be called.\n",
      "NCCL version 2.25.1+cuda12.8\n",
      "[NeMo I 2025-05-09 00:41:07 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:07 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "I0509 07:41:05.809008 338401 pinned_memory_manager.cc:277] \"Pinned memory pool is created at '0x15550c000000' with size 268435456\"\n",
      "I0509 07:41:05.809668 338401 cuda_memory_manager.cc:107] \"CUDA memory pool is created on device 0 with size 67108864\"\n",
      "I0509 07:41:05.813807 338401 server.cc:604] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0509 07:41:05.813828 338401 server.cc:631] \n",
      "+---------+------+--------+\n",
      "| Backend | Path | Config |\n",
      "+---------+------+--------+\n",
      "+---------+------+--------+\n",
      "\n",
      "I0509 07:41:05.813839 338401 server.cc:674] \n",
      "+-------+---------+--------+\n",
      "| Model | Version | Status |\n",
      "+-------+---------+--------+\n",
      "+-------+---------+--------+\n",
      "\n",
      "I0509 07:41:05.854785 338401 metrics.cc:877] \"Collecting metrics for GPU 0: NVIDIA A100-SXM4-80GB\"\n",
      "I0509 07:41:05.862140 338401 metrics.cc:770] \"Collecting CPU metrics\"\n",
      "I0509 07:41:05.862264 338401 tritonserver.cc:2598] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.51.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data parameters statistics trace  |\n",
      "|                                  | logging                                  |\n",
      "| model_repository_path[0]         | /root/.cache/pytriton/workspace_ryedsqek |\n",
      "|                                  | /model-store                             |\n",
      "| model_control_mode               | MODE_EXPLICIT                            |\n",
      "| startup_models_0                 | *                                        |\n",
      "| strict_model_config              | 0                                        |\n",
      "| model_config_name                |                                          |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "| cache_enabled                    | 0                                        |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0509 07:41:05.864735 338401 grpc_server.cc:2558] \"Started GRPCInferenceService at 0.0.0.0:8001\"\n",
      "I0509 07:41:05.864913 338401 http_server.cc:4713] \"Started HTTPService at 0.0.0.0:8000\"\n",
      "I0509 07:41:05.905790 338401 http_server.cc:362] \"Started Metrics Service at 0.0.0.0:8002\"\n",
      "E0509 07:41:06.609621 338401 model_repository_manager.cc:470] \"Failed to set config modification time: model_config_content_name_ is empty\"\n",
      "I0509 07:41:06.609824 338401 model_lifecycle.cc:472] \"loading: triton_model:1\"\n",
      "I0509 07:41:08.276977 338401 python_be.cc:2249] \"TRITONBACKEND_ModelInstanceInitialize: triton_model_0_0 (CPU device 0)\"\n",
      "I0509 07:41:08.856449 338401 model_lifecycle.cc:839] \"successfully loaded 'triton_model'\"\n",
      "[NeMo I 2025-05-09 00:41:08 nemo_logging:393] REST service will be started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['/opt/NeMo/tutorials/llm/evaluation']\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8886 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [338143] using StatReload\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 00:41:09 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:09 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:41:11 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:11 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:41:13 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:13 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:41:15 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:15 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:41:17 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:17 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-09 00:41:19 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 00:41:19 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:19 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n",
      "[NeMo I 2025-05-09 00:41:21 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:21 nemo_logging:393] Pytriton server not ready yet. Retrying in 2 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [338653]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 00:41:23 nemo_logging:393] Checking server and model readiness...\n",
      "[NeMo I 2025-05-09 00:41:23 nemo_logging:393] Attempting to connect to Triton server at: http://0.0.0.0:8000/v2/health/ready\n",
      "INFO:     127.0.0.1:45440 - \"GET /v1/triton_health HTTP/1.1\" 200 OK\n",
      "[NeMo I 2025-05-09 00:41:23 nemo_logging:393] Server is ready.\n",
      "[NeMo I 2025-05-09 00:41:23 nemo_logging:393] Request: model='triton_model' prompt='hello' messages=[{}] max_tokens=1 temperature=1.0 top_p=0.0 top_k=0 logprobs=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-09 00:41:23 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pytriton/client/utils.py:191: NotSupportedTimeoutWarning: Client network and/or connection timeout is smaller than requested timeout_s. This may cause unexpected behavior. network_timeout=600 connection_timeout=600 timeout_s=300\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2025-05-09 00:41:23 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pytriton/client/utils.py:191: NotSupportedTimeoutWarning: Client network and/or connection timeout is smaller than requested timeout_s. This may cause unexpected behavior. network_timeout=600 connection_timeout=600 timeout_s=299.99963665008545\n",
      "      warnings.warn(\n",
      "    \n",
      "static requests:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 00:41:24 nemo_logging:393] Output: {'id': 'cmpl-1746776484', 'object': 'text_completion', 'created': 1746776484, 'model': 'triton_model', 'choices': [{'text': 'you'}]}\n",
      "INFO:     127.0.0.1:45450 - \"POST /v1/completions/ HTTP/1.1\" 200 OK\n",
      "[NeMo I 2025-05-09 00:41:24 nemo_logging:393] Model 'triton_model' is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "static requests: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "2025-05-09:00:41:24,239 INFO     [input.py:188] Invoked config:\n",
      "command='{% if target.api_endpoint.api_key is not none %}OPENAI_API_KEY=${{target.api_endpoint.api_key}}{% endif %} lm-eval --tasks {{config.params.task}}{% if config.params.extra.num_fewshot is defined %} --num_fewshot {{ config.params.extra.num_fewshot }}{% endif %} --model {% if target.api_endpoint.type == \"completions\" %}local-completions{% elif target.api_endpoint.type == \"chat\" %}local-chat-completions{% endif %} --model_args \"base_url={{target.api_endpoint.url}},model={{target.api_endpoint.model_id}},tokenized_requests=false,{% if config.params.extra.tokenizer is not none %}tokenizer={{config.params.extra.tokenizer}}{% endif %},tokenizer_backend={{config.params.extra.tokenizer_backend}},num_concurrent={{config.params.parallelism}},timeout={{ config.params.request_timeout }},max_retries={{ config.params.max_retries }},stream={{ target.api_endpoint.stream }}\" --log_samples --output_path {{config.output_dir}} --use_cache {{config.output_dir}}/lm_cache {% if config.params.limit_samples is not none %}--limit {{config.params.limit_samples}}{% endif %} {% if target.api_endpoint.type == \"chat\" %}--fewshot_as_multiturn --apply_chat_template {% endif %} {% if config.params.extra.args is defined %} {{config.params.extra.args}} {% endif %} {% if config.params.temperature is not none or config.params.top_p is not none or config.params.max_new_tokens is not none %}--gen_kwargs=\"{% if config.params.temperature is not none %}temperature={{ config.params.temperature }}{% endif %}{% if config.params.top_p is not none %},top_p={{ config.params.top_p}}{% endif %}{% if config.params.max_new_tokens is not none %},max_gen_toks={{ config.params.max_new_tokens }}{% endif %}\"{% endif %} {% if config.params.extra.downsampling_ratio is not none %}--downsampling_ratio {{ config.params.extra.downsampling_ratio }}{% endif %}' framework_name='lm-evaluation-harness' pkg_name='lm_eval' config=EvaluationConfig(output_dir='./lambada_openai_results', params=ConfigParams(limit_samples=100, max_new_tokens=None, max_retries=5, parallelism=1, task='lambada_openai', temperature=1e-07, request_timeout=300, top_p=0.9999999, extra={'tokenizer': '/checkpoints/llama-3_2-1b-instruct_v2.0/context/nemo_tokenizer', 'tokenizer_backend': 'huggingface', 'downsampling_ratio': None}), supported_endpoint_types=['completions'], type='lambada_openai') target=EvaluationTarget(api_endpoint=ApiEndpoint(api_key=None, model_id='triton_model', stream=False, type='completions', url='http://0.0.0.0:8886/v1/completions/'))\n",
      "2025-05-09:00:41:24,254 INFO     [evaluate.py:27] Command:  lm-eval --tasks lambada_openai --model local-completions --model_args \"base_url=http://0.0.0.0:8886/v1/completions/,model=triton_model,tokenized_requests=false,tokenizer=/checkpoints/llama-3_2-1b-instruct_v2.0/context/nemo_tokenizer,tokenizer_backend=huggingface,num_concurrent=1,timeout=300,max_retries=5,stream=False\" --log_samples --output_path ./lambada_openai_results --use_cache ./lambada_openai_results/lm_cache --limit 100   --gen_kwargs=\"temperature=1e-07,top_p=0.9999999\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command:  lm-eval --tasks lambada_openai --model local-completions --model_args \"base_url=http://0.0.0.0:8886/v1/completions/,model=triton_model,tokenized_requests=false,tokenizer=/checkpoints/llama-3_2-1b-instruct_v2.0/context/nemo_tokenizer,tokenizer_backend=huggingface,num_concurrent=1,timeout=300,max_retries=5,stream=False\" --log_samples --output_path ./lambada_openai_results --use_cache ./lambada_openai_results/lm_cache --limit 100   --gen_kwargs=\"temperature=1e-07,top_p=0.9999999\" \n",
      "Temporary directory created at: /tmp/tmplgbl06yy\n",
      "Script file created: /tmp/tmplgbl06yy/39a542857e41b861821e3bda44ef8ddf27b92181.sh\n",
      "Command written to script file.\n",
      "Subprocess started.\n",
      "2025-05-09:00:41:28,588 INFO     [__main__.py:337] Verbosity set to INFO\n",
      "2025-05-09:00:41:34,795 WARNING  [__main__.py:370]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2025-05-09:00:41:34,796 INFO     [__main__.py:444] Selected Tasks: ['lambada_openai']\n",
      "2025-05-09:00:41:34,796 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-05-09:00:41:34,796 WARNING  [evaluator.py:175] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2025-05-09:00:41:34,796 INFO     [evaluator.py:201] Initializing local-completions model, with arguments: {'base_url': 'http://0.0.0.0:8886/v1/completions/', 'model': 'triton_model', 'tokenized_requests': False, 'tokenizer': '/checkpoints/llama-3_2-1b-instruct_v2.0/context/nemo_tokenizer', 'tokenizer_backend': 'huggingface', 'num_concurrent': 1, 'timeout': 300, 'max_retries': 5, 'stream': False}\n",
      "2025-05-09:00:41:34,796 INFO     [api_models.py:116] Using max length 2048 - 1\n",
      "2025-05-09:00:41:34,796 INFO     [api_models.py:119] Concurrent requests are disabled. To enable concurrent requests, set `num_concurrent` > 1.\n",
      "2025-05-09:00:41:34,796 INFO     [api_models.py:135] Using tokenizer huggingface\n",
      "2025-05-09:00:41:35,192 INFO     [evaluator.py:229] Using cache at ./lambada_openai_results/lm_cache_rank0.db\n",
      "2025-05-09:00:41:37,387 WARNING  [task.py:328] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-05-09:00:41:37,387 WARNING  [task.py:328] [Task: lambada_openai] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2025-05-09:00:41:37,449 INFO     [task.py:421] Building contexts for lambada_openai on rank 0...\n",
      "100% 100/100 [00:00<00:00, 729.24it/s]\n",
      "2025-05-09:00:41:37,587 INFO     [evaluator.py:509] Running loglikelihood requests\n",
      "2025-05-09:00:41:37,587 INFO     [model.py:359] Loading 'loglikelihood' responses from cache './lambada_openai_results/lm_cache_rank0.db' where possible...\n",
      "Checking cached requests: 100% 100/100 [00:00<00:00, 638.98it/s]\n",
      "2025-05-09:00:41:37,744 INFO     [model.py:396] Cached requests: 100, Requests remaining: 0\n",
      "bootstrapping for stddev: perplexity\n",
      "100% 100/100 [00:00<00:00, 1044.86it/s]\n",
      "2025-05-09:00:41:40,031 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "2025-05-09:00:41:40,034 INFO     [evaluation_tracker.py:287] Saving per-sample results for: lambada_openai\n",
      "local-completions (base_url=http://0.0.0.0:8886/v1/completions/,model=triton_model,tokenized_requests=false,tokenizer=/checkpoints/llama-3_2-1b-instruct_v2.0/context/nemo_tokenizer,tokenizer_backend=huggingface,num_concurrent=1,timeout=300,max_retries=5,stream=False), gen_kwargs: (temperature=1e-07,top_p=0.9999999), limit: 100.0, num_fewshot: None, batch_size: 1\n",
      "|    Tasks     |Version|Filter|n-shot|  Metric  |   |Value|   |Stderr|\n",
      "|--------------|------:|------|-----:|----------|---|----:|---|-----:|\n",
      "|lambada_openai|      1|none  |     0|acc       |↑  |    1|±  |     0|\n",
      "|              |       |none  |     0|perplexity|↓  |    1|±  |     0|\n",
      "\n",
      "Output reading completed.\n",
      "Subprocess finished with return code: 0\n",
      "[NeMo I 2025-05-09 00:41:41 nemo_logging:393] ========== RESULTS ==========\n",
      "[NeMo I 2025-05-09 00:41:41 nemo_logging:393] groups:\n",
      "      lambada_openai:\n",
      "        groups: null\n",
      "        metrics:\n",
      "          acc:\n",
      "            scores:\n",
      "              acc:\n",
      "                stats:\n",
      "                  count: null\n",
      "                  max: null\n",
      "                  mean: null\n",
      "                  min: null\n",
      "                  stddev: null\n",
      "                  stderr: 0.0\n",
      "                  sum: null\n",
      "                  sum_squared: null\n",
      "                  variance: null\n",
      "                value: 1.0\n",
      "          perplexity:\n",
      "            scores:\n",
      "              perplexity:\n",
      "                stats:\n",
      "                  count: null\n",
      "                  max: null\n",
      "                  mean: null\n",
      "                  min: null\n",
      "                  stddev: null\n",
      "                  stderr: 0.0\n",
      "                  sum: null\n",
      "                  sum_squared: null\n",
      "                  variance: null\n",
      "                value: 1.0\n",
      "    tasks:\n",
      "      lambada_openai:\n",
      "        metrics:\n",
      "          acc:\n",
      "            scores:\n",
      "              acc:\n",
      "                stats:\n",
      "                  count: null\n",
      "                  max: null\n",
      "                  mean: null\n",
      "                  min: null\n",
      "                  stddev: null\n",
      "                  stderr: 0.0\n",
      "                  sum: null\n",
      "                  sum_squared: null\n",
      "                  variance: null\n",
      "                value: 1.0\n",
      "          perplexity:\n",
      "            scores:\n",
      "              perplexity:\n",
      "                stats:\n",
      "                  count: null\n",
      "                  max: null\n",
      "                  mean: null\n",
      "                  min: null\n",
      "                  stddev: null\n",
      "                  stderr: 0.0\n",
      "                  sum: null\n",
      "                  sum_squared: null\n",
      "                  variance: null\n",
      "                value: 1.0\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "target_config = EvaluationTarget(api_endpoint={\"url\": completions_url, \"type\": \"completions\"})\n",
    "eval_config = EvaluationConfig(\n",
    "    type=\"lm-evaluation-harness.lambada_openai\",\n",
    "    params={\"limit_samples\": 100,  # remove to run on whole dataset; set to float from 0-1 range to use fraction of data\n",
    "            \"extra\": {\n",
    "                \"tokenizer_backend\": \"huggingface\",\n",
    "                \"tokenizer\": f\"{CHECKPOINT_PATH}/context/nemo_tokenizer\"},\n",
    "           },\n",
    "    output_dir=f\"{WORKSPACE}/lambada_openai_results\",\n",
    ")\n",
    "\n",
    "results = api.evaluate(target_cfg=target_config, eval_cfg=eval_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55c94d0-f302-48d1-bb40-8cbdea323e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_process.send_signal(signal.SIGINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac073b9-f581-4df3-8225-92086a2a0962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': {'lambada_openai': {'metrics': {'perplexity': {'scores': {'perplexity': {'value': 1.0,\n",
       "       'stats': {'count': None,\n",
       "        'sum': None,\n",
       "        'sum_squared': None,\n",
       "        'min': None,\n",
       "        'max': None,\n",
       "        'mean': None,\n",
       "        'variance': None,\n",
       "        'stddev': None,\n",
       "        'stderr': 0.0}}}},\n",
       "    'acc': {'scores': {'acc': {'value': 1.0,\n",
       "       'stats': {'count': None,\n",
       "        'sum': None,\n",
       "        'sum_squared': None,\n",
       "        'min': None,\n",
       "        'max': None,\n",
       "        'mean': None,\n",
       "        'variance': None,\n",
       "        'stddev': None,\n",
       "        'stderr': 0.0}}}}}}},\n",
       " 'groups': {'lambada_openai': {'groups': None,\n",
       "   'metrics': {'perplexity': {'scores': {'perplexity': {'value': 1.0,\n",
       "       'stats': {'count': None,\n",
       "        'sum': None,\n",
       "        'sum_squared': None,\n",
       "        'min': None,\n",
       "        'max': None,\n",
       "        'mean': None,\n",
       "        'variance': None,\n",
       "        'stddev': None,\n",
       "        'stderr': 0.0}}}},\n",
       "    'acc': {'scores': {'acc': {'value': 1.0,\n",
       "       'stats': {'count': None,\n",
       "        'sum': None,\n",
       "        'sum_squared': None,\n",
       "        'min': None,\n",
       "        'max': None,\n",
       "        'mean': None,\n",
       "        'variance': None,\n",
       "        'stddev': None,\n",
       "        'stderr': 0.0}}}}}}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeba60b3-767c-4c89-b6b2-93f7c67f1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ./lambada_openai_results/triton_model/samples_lambada_openai_2025-05-09T00-39-02.119298.jsonl <==\n",
      "{\"doc_id\": 0, \"doc\": {\"text\": \"In my palm is a clear stone, and inside it is a small ivory statuette. A guardian angel.\\n\\n\\\"Figured if you're going to be out at night getting hit by cars, you might as well have some backup.\\\"\\n\\nI look at him, feeling stunned. Like this is some sort of sign. But as I stare at Harlin, his mouth curved in a confident grin, I don't care about signs\"}, \"target\": \" signs\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"In my palm is a clear stone, and inside it is a small ivory statuette. A guardian angel.\\n\\n\\\"Figured if you're going to be out at night getting hit by cars, you might as well have some backup.\\\"\\n\\nI look at him, feeling stunned. Like this is some sort of sign. But as I stare at Harlin, his mouth curved in a confident grin, I don't care about\", \"arg_1\": \" signs\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"d5d2ee0c7bb2325b1a3b782dd2f946222c370109812c56acc7da70b662a6a60c\", \"prompt_hash\": \"8d638394e95544a75ea75bfa8cedd5fb71d5e65dfac9758cad35a0d5fa6e296c\", \"target_hash\": \"697dfd086e17614160f34474ccf8681ad12ae5839865fc979ac177eb5bba0a48\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 1, \"doc\": {\"text\": \"Give me a minute to change and I'll meet you at the docks.\\\" She'd forced those words through her teeth.\\n\\n\\\"No need to change. We won't be that long.\\\"\\n\\nShane gripped her arm and started leading her to the dock.\\n\\n\\\"I can make it there on my own, Shane\"}, \"target\": \" Shane\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"Give me a minute to change and I'll meet you at the docks.\\\" She'd forced those words through her teeth.\\n\\n\\\"No need to change. We won't be that long.\\\"\\n\\nShane gripped her arm and started leading her to the dock.\\n\\n\\\"I can make it there on my own,\", \"arg_1\": \" Shane\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"a0a3a6af029a08c1101838dad7c9d14d25760f6bf91903c29a29ed93aca182c7\", \"prompt_hash\": \"4bf9c8952b622b3e634fed1df7684bf3857fbe89b76472483ac12c88672c6d0d\", \"target_hash\": \"aae27ee803ccb2af84968ea406180b5a8a91494318ca531aa757eb9ff1a74355\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 2, \"doc\": {\"text\": \"\\\"Only one source I know of that would be likely to cough up enough money to finance a phony sleep research facility and pay people big bucks to solve crimes in their dreams,\\\" Farrell concluded dryly.\\n\\n\\\"What can I say?\\\" Ellis unfolded his arms and widened his hands. \\\"Your tax dollars at work.\\\"\\n\\nBefore Farrell could respond, Leila's voice rose from inside the house.\\n\\n\\\"No insurance?\\\" she wailed. \\\"What do you mean you don't have any insurance\"}, \"target\": \" insurance\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"\\\"Only one source I know of that would be likely to cough up enough money to finance a phony sleep research facility and pay people big bucks to solve crimes in their dreams,\\\" Farrell concluded dryly.\\n\\n\\\"What can I say?\\\" Ellis unfolded his arms and widened his hands. \\\"Your tax dollars at work.\\\"\\n\\nBefore Farrell could respond, Leila's voice rose from inside the house.\\n\\n\\\"No insurance?\\\" she wailed. \\\"What do you mean you don't have any\", \"arg_1\": \" insurance\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"256607fe6ede8f60d3e885528a1ffe42cc84c8341195a130589e5157e831917d\", \"prompt_hash\": \"1288614b8104fe7fcb51b6d2a46735afb98adcdb1001bb3550a9307e8f7cfbb1\", \"target_hash\": \"4afb6c58c051ff6e651037f1cd766b456b4926d4661c2ff3eb1b1308c390375d\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 3, \"doc\": {\"text\": \"Helen's heart broke a little in the face of Miss Mabel's selfless courage. She thought that because she was old, her life was of less value than the others'. For all Helen knew, Miss Mabel had a lot more years to live than she did. \\\"Not going to happen,\\\" replied Helen\"}, \"target\": \" Helen\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"Helen's heart broke a little in the face of Miss Mabel's selfless courage. She thought that because she was old, her life was of less value than the others'. For all Helen knew, Miss Mabel had a lot more years to live than she did. \\\"Not going to happen,\\\" replied\", \"arg_1\": \" Helen\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"e743cba45b3b5042ac2d7507aca6ff092848fa9e4c528dbe91fbbe3437dea223\", \"prompt_hash\": \"3588dd8590897c7f63d1aabb25c76f026c8a03520279e74fc1a977b65d40d65f\", \"target_hash\": \"09ca8ddec715d10f41c2878897f2fd4b82145ddd8e64873b253bd5ac5c84dfaf\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 4, \"doc\": {\"text\": \"Preston had been the last person to wear those chains, and I knew what I'd see and feel if they were slipped onto my skin-the Reaper's unending hatred of me. I'd felt enough of that emotion already in the amphitheater. I didn't want to feel anymore.\\n\\n\\\"Don't put those on me,\\\" I whispered. \\\"Please.\\\"\\n\\nSergei looked at me, surprised by my low, raspy please, but he put down the chains\"}, \"target\": \" chains\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"Preston had been the last person to wear those chains, and I knew what I'd see and feel if they were slipped onto my skin-the Reaper's unending hatred of me. I'd felt enough of that emotion already in the amphitheater. I didn't want to feel anymore.\\n\\n\\\"Don't put those on me,\\\" I whispered. \\\"Please.\\\"\\n\\nSergei looked at me, surprised by my low, raspy please, but he put down the\", \"arg_1\": \" chains\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"d15f8de8aea23dddee3d692b4efeabd2b91e50680aaaf184c9e52a329ce83359\", \"prompt_hash\": \"8bc810169eb8987c4fbd36e21716e7894c19441ac82505b8be5fe594e0c73f69\", \"target_hash\": \"2ef00f49157b9201cc7e302ed59346520ba5be2d0f89f1de4674a05e9d97eec9\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 5, \"doc\": {\"text\": \"She knew that Basha was a decent young man, that he was pretty sweet and friendly with her. Jawen knew they had a bit of a history, but she thought that this time she would get along better with him, that she could overlook those problems. \\nThey kissed, and she knew that she liked Basha, but then Hastin interfered. She was so angry that she immediately said, once they were out of earshot of Basha, “You don’t mean anything to me anymore, Hastin\"}, \"target\": \" Hastin\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"She knew that Basha was a decent young man, that he was pretty sweet and friendly with her. Jawen knew they had a bit of a history, but she thought that this time she would get along better with him, that she could overlook those problems. \\nThey kissed, and she knew that she liked Basha, but then Hastin interfered. She was so angry that she immediately said, once they were out of earshot of Basha, “You don’t mean anything to me anymore,\", \"arg_1\": \" Hastin\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"9acbd94fd9c1fcffd8d709269d7658cbeb2c5a7a178202f550258dda1d0665b8\", \"prompt_hash\": \"e333fe1b429b240086c3d34f1756961ffec8372d29cc2d83fbeb67972244e2a9\", \"target_hash\": \"e105d3fed9f1dcc2884085993c1db050a5c58ce9fb77cd62da141df0d2804a19\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 6, \"doc\": {\"text\": \"He heard Rhinna speak “The Queen wants you in her carriage.” \\nTom spoke “No, I’m not going in some asylum.” \\nRan was seen standing next to him spoke “It’s just for a private talk with you that’s all.” \\nTom groaned and went inside the carriage to sit down next to the Queen\"}, \"target\": \" Queen\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"He heard Rhinna speak “The Queen wants you in her carriage.” \\nTom spoke “No, I’m not going in some asylum.” \\nRan was seen standing next to him spoke “It’s just for a private talk with you that’s all.” \\nTom groaned and went inside the carriage to sit down next to the\", \"arg_1\": \" Queen\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"2b47158a546f0265e83c8b24cecd08bddac6f500e2dfe8e5dcf7ad3b794b7a9b\", \"prompt_hash\": \"1674fd043ccb89ef9297e57d60b82eeffc11209abf75cb86843108837ea8ccd1\", \"target_hash\": \"a2d35fb361f2cdc938561c4c9ea0bceb6a7a29d1fbf663a3ed5359219b5f82af\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 7, \"doc\": {\"text\": \"There was no way he would come here on his own. \\nHe ordered a cup of coffee, and then we just sat in silence. \\n“So,” Aidan finally said, “How’s it going?”\\nI laughed. “Not much has changed since the last time I saw you.”\\n“Ya know, you eat here a lot,” said Aidan\"}, \"target\": \" Aidan\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"There was no way he would come here on his own. \\nHe ordered a cup of coffee, and then we just sat in silence. \\n“So,” Aidan finally said, “How’s it going?”\\nI laughed. “Not much has changed since the last time I saw you.”\\n“Ya know, you eat here a lot,” said\", \"arg_1\": \" Aidan\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"ecf61b72e8c1a8967302b92c5e95a893f0bb7c6b38df2db8773d9540543366ec\", \"prompt_hash\": \"83484906f28829a02382a4055f089d31a0513f7d8678cde984be346207fa329d\", \"target_hash\": \"cfc1a96cf58107d77cc8df5885fb6d68701c878a5ebafba2a032cd0c5e915c8e\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 8, \"doc\": {\"text\": \"“Why?”\\n“I would have thought you’d find him rather dry,” she said.\\n“I don’t know about that,” said Gabriel.\\n“He was a great craftsman,” said Heather.\\n“That he was,” said Flannery.\\n“And Polish, to boot,” said Gabriel\"}, \"target\": \" Gabriel\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"“Why?”\\n“I would have thought you’d find him rather dry,” she said.\\n“I don’t know about that,” said Gabriel.\\n“He was a great craftsman,” said Heather.\\n“That he was,” said Flannery.\\n“And Polish, to boot,” said\", \"arg_1\": \" Gabriel\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"b8ecb9bb71a86a61369a95e4b84f89c378e8420ba5898e7e3c65ce0e7f583bc0\", \"prompt_hash\": \"2ebbc6b1841cf5269fc9db53d72d9c9e8b616e134958862c7fdb068a3c4fb95d\", \"target_hash\": \"a8f354d7bdf92a493b41beac6c2ad5779a496721d89110e96796564301c241a0\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 9, \"doc\": {\"text\": \"Both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen, and I was glad to relax against the tree’s rough, brittle bark and begin my breakfast of buttery, toasted bread and fresh fruit. Even the water was tasty, it was so clean and cold. It almost made up for the lack of coffee\"}, \"target\": \" coffee\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"Both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen, and I was glad to relax against the tree’s rough, brittle bark and begin my breakfast of buttery, toasted bread and fresh fruit. Even the water was tasty, it was so clean and cold. It almost made up for the lack of\", \"arg_1\": \" coffee\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"af460259d650b6fd120fc09d97c6e227facbd499e846bea2793d1d647da7f807\", \"prompt_hash\": \"98dc90a1352c33e1b5dea88238cb86e19d001386ca63b38f8f88d070d7289c74\", \"target_hash\": \"9d95b4b2dc23d7b767ec69d74fb4d17ea634ea61f561de561c482d557e2d293b\", \"perplexity\": 0, \"acc\": 1}\n",
      "\n",
      "==> ./lambada_openai_results/triton_model/samples_lambada_openai_2025-05-09T00-41-40.031733.jsonl <==\n",
      "{\"doc_id\": 0, \"doc\": {\"text\": \"In my palm is a clear stone, and inside it is a small ivory statuette. A guardian angel.\\n\\n\\\"Figured if you're going to be out at night getting hit by cars, you might as well have some backup.\\\"\\n\\nI look at him, feeling stunned. Like this is some sort of sign. But as I stare at Harlin, his mouth curved in a confident grin, I don't care about signs\"}, \"target\": \" signs\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"In my palm is a clear stone, and inside it is a small ivory statuette. A guardian angel.\\n\\n\\\"Figured if you're going to be out at night getting hit by cars, you might as well have some backup.\\\"\\n\\nI look at him, feeling stunned. Like this is some sort of sign. But as I stare at Harlin, his mouth curved in a confident grin, I don't care about\", \"arg_1\": \" signs\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"d5d2ee0c7bb2325b1a3b782dd2f946222c370109812c56acc7da70b662a6a60c\", \"prompt_hash\": \"8d638394e95544a75ea75bfa8cedd5fb71d5e65dfac9758cad35a0d5fa6e296c\", \"target_hash\": \"697dfd086e17614160f34474ccf8681ad12ae5839865fc979ac177eb5bba0a48\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 1, \"doc\": {\"text\": \"Give me a minute to change and I'll meet you at the docks.\\\" She'd forced those words through her teeth.\\n\\n\\\"No need to change. We won't be that long.\\\"\\n\\nShane gripped her arm and started leading her to the dock.\\n\\n\\\"I can make it there on my own, Shane\"}, \"target\": \" Shane\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"Give me a minute to change and I'll meet you at the docks.\\\" She'd forced those words through her teeth.\\n\\n\\\"No need to change. We won't be that long.\\\"\\n\\nShane gripped her arm and started leading her to the dock.\\n\\n\\\"I can make it there on my own,\", \"arg_1\": \" Shane\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"a0a3a6af029a08c1101838dad7c9d14d25760f6bf91903c29a29ed93aca182c7\", \"prompt_hash\": \"4bf9c8952b622b3e634fed1df7684bf3857fbe89b76472483ac12c88672c6d0d\", \"target_hash\": \"aae27ee803ccb2af84968ea406180b5a8a91494318ca531aa757eb9ff1a74355\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 2, \"doc\": {\"text\": \"\\\"Only one source I know of that would be likely to cough up enough money to finance a phony sleep research facility and pay people big bucks to solve crimes in their dreams,\\\" Farrell concluded dryly.\\n\\n\\\"What can I say?\\\" Ellis unfolded his arms and widened his hands. \\\"Your tax dollars at work.\\\"\\n\\nBefore Farrell could respond, Leila's voice rose from inside the house.\\n\\n\\\"No insurance?\\\" she wailed. \\\"What do you mean you don't have any insurance\"}, \"target\": \" insurance\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"\\\"Only one source I know of that would be likely to cough up enough money to finance a phony sleep research facility and pay people big bucks to solve crimes in their dreams,\\\" Farrell concluded dryly.\\n\\n\\\"What can I say?\\\" Ellis unfolded his arms and widened his hands. \\\"Your tax dollars at work.\\\"\\n\\nBefore Farrell could respond, Leila's voice rose from inside the house.\\n\\n\\\"No insurance?\\\" she wailed. \\\"What do you mean you don't have any\", \"arg_1\": \" insurance\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"256607fe6ede8f60d3e885528a1ffe42cc84c8341195a130589e5157e831917d\", \"prompt_hash\": \"1288614b8104fe7fcb51b6d2a46735afb98adcdb1001bb3550a9307e8f7cfbb1\", \"target_hash\": \"4afb6c58c051ff6e651037f1cd766b456b4926d4661c2ff3eb1b1308c390375d\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 3, \"doc\": {\"text\": \"Helen's heart broke a little in the face of Miss Mabel's selfless courage. She thought that because she was old, her life was of less value than the others'. For all Helen knew, Miss Mabel had a lot more years to live than she did. \\\"Not going to happen,\\\" replied Helen\"}, \"target\": \" Helen\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"Helen's heart broke a little in the face of Miss Mabel's selfless courage. She thought that because she was old, her life was of less value than the others'. For all Helen knew, Miss Mabel had a lot more years to live than she did. \\\"Not going to happen,\\\" replied\", \"arg_1\": \" Helen\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"e743cba45b3b5042ac2d7507aca6ff092848fa9e4c528dbe91fbbe3437dea223\", \"prompt_hash\": \"3588dd8590897c7f63d1aabb25c76f026c8a03520279e74fc1a977b65d40d65f\", \"target_hash\": \"09ca8ddec715d10f41c2878897f2fd4b82145ddd8e64873b253bd5ac5c84dfaf\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 4, \"doc\": {\"text\": \"Preston had been the last person to wear those chains, and I knew what I'd see and feel if they were slipped onto my skin-the Reaper's unending hatred of me. I'd felt enough of that emotion already in the amphitheater. I didn't want to feel anymore.\\n\\n\\\"Don't put those on me,\\\" I whispered. \\\"Please.\\\"\\n\\nSergei looked at me, surprised by my low, raspy please, but he put down the chains\"}, \"target\": \" chains\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"Preston had been the last person to wear those chains, and I knew what I'd see and feel if they were slipped onto my skin-the Reaper's unending hatred of me. I'd felt enough of that emotion already in the amphitheater. I didn't want to feel anymore.\\n\\n\\\"Don't put those on me,\\\" I whispered. \\\"Please.\\\"\\n\\nSergei looked at me, surprised by my low, raspy please, but he put down the\", \"arg_1\": \" chains\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"d15f8de8aea23dddee3d692b4efeabd2b91e50680aaaf184c9e52a329ce83359\", \"prompt_hash\": \"8bc810169eb8987c4fbd36e21716e7894c19441ac82505b8be5fe594e0c73f69\", \"target_hash\": \"2ef00f49157b9201cc7e302ed59346520ba5be2d0f89f1de4674a05e9d97eec9\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 5, \"doc\": {\"text\": \"She knew that Basha was a decent young man, that he was pretty sweet and friendly with her. Jawen knew they had a bit of a history, but she thought that this time she would get along better with him, that she could overlook those problems. \\nThey kissed, and she knew that she liked Basha, but then Hastin interfered. She was so angry that she immediately said, once they were out of earshot of Basha, “You don’t mean anything to me anymore, Hastin\"}, \"target\": \" Hastin\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"She knew that Basha was a decent young man, that he was pretty sweet and friendly with her. Jawen knew they had a bit of a history, but she thought that this time she would get along better with him, that she could overlook those problems. \\nThey kissed, and she knew that she liked Basha, but then Hastin interfered. She was so angry that she immediately said, once they were out of earshot of Basha, “You don’t mean anything to me anymore,\", \"arg_1\": \" Hastin\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"9acbd94fd9c1fcffd8d709269d7658cbeb2c5a7a178202f550258dda1d0665b8\", \"prompt_hash\": \"e333fe1b429b240086c3d34f1756961ffec8372d29cc2d83fbeb67972244e2a9\", \"target_hash\": \"e105d3fed9f1dcc2884085993c1db050a5c58ce9fb77cd62da141df0d2804a19\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 6, \"doc\": {\"text\": \"He heard Rhinna speak “The Queen wants you in her carriage.” \\nTom spoke “No, I’m not going in some asylum.” \\nRan was seen standing next to him spoke “It’s just for a private talk with you that’s all.” \\nTom groaned and went inside the carriage to sit down next to the Queen\"}, \"target\": \" Queen\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"He heard Rhinna speak “The Queen wants you in her carriage.” \\nTom spoke “No, I’m not going in some asylum.” \\nRan was seen standing next to him spoke “It’s just for a private talk with you that’s all.” \\nTom groaned and went inside the carriage to sit down next to the\", \"arg_1\": \" Queen\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"2b47158a546f0265e83c8b24cecd08bddac6f500e2dfe8e5dcf7ad3b794b7a9b\", \"prompt_hash\": \"1674fd043ccb89ef9297e57d60b82eeffc11209abf75cb86843108837ea8ccd1\", \"target_hash\": \"a2d35fb361f2cdc938561c4c9ea0bceb6a7a29d1fbf663a3ed5359219b5f82af\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 7, \"doc\": {\"text\": \"There was no way he would come here on his own. \\nHe ordered a cup of coffee, and then we just sat in silence. \\n“So,” Aidan finally said, “How’s it going?”\\nI laughed. “Not much has changed since the last time I saw you.”\\n“Ya know, you eat here a lot,” said Aidan\"}, \"target\": \" Aidan\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"There was no way he would come here on his own. \\nHe ordered a cup of coffee, and then we just sat in silence. \\n“So,” Aidan finally said, “How’s it going?”\\nI laughed. “Not much has changed since the last time I saw you.”\\n“Ya know, you eat here a lot,” said\", \"arg_1\": \" Aidan\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"ecf61b72e8c1a8967302b92c5e95a893f0bb7c6b38df2db8773d9540543366ec\", \"prompt_hash\": \"83484906f28829a02382a4055f089d31a0513f7d8678cde984be346207fa329d\", \"target_hash\": \"cfc1a96cf58107d77cc8df5885fb6d68701c878a5ebafba2a032cd0c5e915c8e\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 8, \"doc\": {\"text\": \"“Why?”\\n“I would have thought you’d find him rather dry,” she said.\\n“I don’t know about that,” said Gabriel.\\n“He was a great craftsman,” said Heather.\\n“That he was,” said Flannery.\\n“And Polish, to boot,” said Gabriel\"}, \"target\": \" Gabriel\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"“Why?”\\n“I would have thought you’d find him rather dry,” she said.\\n“I don’t know about that,” said Gabriel.\\n“He was a great craftsman,” said Heather.\\n“That he was,” said Flannery.\\n“And Polish, to boot,” said\", \"arg_1\": \" Gabriel\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"b8ecb9bb71a86a61369a95e4b84f89c378e8420ba5898e7e3c65ce0e7f583bc0\", \"prompt_hash\": \"2ebbc6b1841cf5269fc9db53d72d9c9e8b616e134958862c7fdb068a3c4fb95d\", \"target_hash\": \"a8f354d7bdf92a493b41beac6c2ad5779a496721d89110e96796564301c241a0\", \"perplexity\": 0, \"acc\": 1}\n",
      "{\"doc_id\": 9, \"doc\": {\"text\": \"Both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen, and I was glad to relax against the tree’s rough, brittle bark and begin my breakfast of buttery, toasted bread and fresh fruit. Even the water was tasty, it was so clean and cold. It almost made up for the lack of coffee\"}, \"target\": \" coffee\", \"arguments\": {\"gen_args_0\": {\"arg_0\": \"Both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen, and I was glad to relax against the tree’s rough, brittle bark and begin my breakfast of buttery, toasted bread and fresh fruit. Even the water was tasty, it was so clean and cold. It almost made up for the lack of\", \"arg_1\": \" coffee\"}}, \"resps\": [[[\"0\", \"True\"]]], \"filtered_resps\": [[\"0\", \"True\"]], \"filter\": \"none\", \"metrics\": [\"perplexity\", \"acc\"], \"doc_hash\": \"af460259d650b6fd120fc09d97c6e227facbd499e846bea2793d1d647da7f807\", \"prompt_hash\": \"98dc90a1352c33e1b5dea88238cb86e19d001386ca63b38f8f88d070d7289c74\", \"target_hash\": \"9d95b4b2dc23d7b767ec69d74fb4d17ea634ea61f561de561c482d557e2d293b\", \"perplexity\": 0, \"acc\": 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [338653]\n",
      "INFO:     Stopping reloader process [338143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-09 00:41:41 nemo_logging:393] Model serving on Triton will be started.\n"
     ]
    }
   ],
   "source": [
    "! head {WORKSPACE}/lambada_openai_results/triton_model/samples_lambada_openai_*.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78efa6f2-4dc2-49bd-afa1-9d89e5f09fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
